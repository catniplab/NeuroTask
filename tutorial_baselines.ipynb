{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_neurotask import *\n",
    "from baselines.metrics import get_R2\n",
    "from baselines.decoders import *\n",
    "from baselines.preprocessing_funcs import *\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file_path = 'NeuroTask/2_1_Chowdhury_CObump.parquet'\n",
    "df = load_and_filter_parquet(parquet_file_path,['A', 'I','F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = rebin(df,bin_size=20,reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = [col for col in df10.columns if col.startswith('Neuron')]\n",
    "X_train_list = []\n",
    "X_test_list = []\n",
    "X_val_list = []\n",
    "\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "y_val_list = []\n",
    "\n",
    "bins_before=5 #How many bins of neural data prior to the output are used for decoding\n",
    "bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "training_range=[0, 0.7]\n",
    "testing_range=[0.8, 1]\n",
    "valid_range=[0.7,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each unique animal in the dataset\n",
    "for a in df10['animal'].unique():\n",
    "    # Select data for the current animal\n",
    "    d = df10[(df10['animal'] == a)]\n",
    "\n",
    "    # Calculate the total number of sessions for the current animal\n",
    "    n_samples = d['session'].value_counts().max() \n",
    "\n",
    "    # Iterate over each session for the current animal\n",
    "    for session in d['session'].unique():\n",
    "\n",
    "        # Select data for the current session and filter out zero columns\n",
    "        df_session = df10[(df10['animal'] == a) & (df10['session'] == session)][neurons].dropna(axis=1)\n",
    "        df_session = df_session.loc[:, (df_session != 0).any(axis=0)]\n",
    "\n",
    "        # Extract hand/cursor velocity data for the current session\n",
    "        y = np.array(df10[ (df10['session'] == session) & (df10['animal'] == a)][['hand_vel_x', 'hand_vel_y']])\n",
    "        \n",
    "        # Convert DataFrame to NumPy array and transpose\n",
    "        session_data = df_session.to_numpy()  # Transpose to have (n_features, n_samples)\n",
    "\n",
    "        # Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "        # Function to get the covariate matrix that includes spike history from previous bins\n",
    "        X=get_spikes_with_history(session_data,bins_before,bins_after,bins_current)\n",
    "        num_examples=X.shape[0]\n",
    "\n",
    "        #Note that each range has a buffer of\"bins_before\" bins at the beginning, and \"bins_after\" bins at the end\n",
    "        #This makes it so that the different sets don't include overlapping neural data\n",
    "        training_set=np.arange(int(np.round(training_range[0]*num_examples))+bins_before,int(np.round(training_range[1]*num_examples))-bins_after)\n",
    "        testing_set=np.arange(int(np.round(testing_range[0]*num_examples))+bins_before,int(np.round(testing_range[1]*num_examples))-bins_after)\n",
    "        valid_set=np.arange(int(np.round(valid_range[0]*num_examples))+bins_before,int(np.round(valid_range[1]*num_examples))-bins_after)\n",
    "\n",
    "        #Get training data\n",
    "        X_train=X[training_set,:,:]\n",
    "        y_train=y[training_set,:]\n",
    "\n",
    "        #Get testing data\n",
    "        X_test=X[testing_set,:,:]\n",
    "        y_test=y[testing_set,:]\n",
    "\n",
    "        #Get validation data\n",
    "        X_valid=X[valid_set,:,:]\n",
    "        y_valid=y[valid_set,:]\n",
    "\n",
    "        #Z-score \"X\" inputs. \n",
    "        X_train_mean=np.nanmean(X_train,axis=0)\n",
    "        X_train_std=np.nanstd(X_train,axis=0)\n",
    "        X_train_std = np.where(X_train_std == 0, 1e-16, X_train_std)\n",
    "\n",
    "        X_train=(X_train-X_train_mean)/X_train_std\n",
    "        X_test=(X_test-X_train_mean)/X_train_std\n",
    "        X_valid=(X_valid-X_train_mean)/X_train_std\n",
    "\n",
    "        #Zero-center outputs\n",
    "        y_train_mean=np.mean(y_train,axis=0)\n",
    "        y_train=y_train-y_train_mean\n",
    "        y_test=y_test-y_train_mean\n",
    "        y_valid=y_valid-y_train_mean\n",
    "\n",
    "        X_train_list.append(X_train)\n",
    "        X_test_list.append(X_test)\n",
    "        X_val_list.append(X_valid)\n",
    "        \n",
    "\n",
    "        y_train_list.append(y_train)\n",
    "        y_test_list.append(y_test)\n",
    "        y_val_list.append(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", sum(X.shape[2] for X in X_train_list), \"unique neurons in the dataset in\",len(X_train_list),\"different sessions\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiener Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train_list)):\n",
    "    X_flat_train=X_train_list[i].reshape(X_train_list[i].shape[0],(X_train_list[i].shape[1]*X_train_list[i].shape[2]))\n",
    "\n",
    "    X_flat_test = X_test_list[i].reshape(X_test_list[i].shape[0],(X_test_list[i].shape[1]*X_test_list[i].shape[2]))\n",
    "\n",
    "    #Declare model\n",
    "    model_wf=WienerFilterDecoder()\n",
    "\n",
    "    #Fit model|\n",
    "    model_wf.fit(X_flat_train,y_train_list[i])\n",
    "\n",
    "    #Get predictions\n",
    "    y_valid_predicted_wf=model_wf.predict(X_flat_test)\n",
    "\n",
    "    #Get metric of fit\n",
    "    R2s_wf=get_R2(y_test_list[i],y_valid_predicted_wf)\n",
    "    print('session', i+1,'R2s:', R2s_wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiener Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train_list)):\n",
    "    #for i in range(len(X_train_list)):\n",
    "    X_flat_train=X_train_list[i].reshape(X_train_list[i].shape[0],(X_train_list[i].shape[1]*X_train_list[i].shape[2]))\n",
    "\n",
    "    X_flat_test = X_test_list[i].reshape(X_test_list[i].shape[0],(X_test_list[i].shape[1]*X_test_list[i].shape[2]))\n",
    "\n",
    "    #Declare model\n",
    "    model_wf=WienerCascadeDecoder(degree=3)\n",
    "\n",
    "    #Fit model\n",
    "    model_wf.fit(X_flat_train,y_train_list[i])\n",
    "\n",
    "    #Get predictions\n",
    "    y_valid_predicted_wf=model_wf.predict(X_flat_test)\n",
    "\n",
    "    #Get metric of fit\n",
    "    R2s_wf=get_R2(y_test_list[i],y_valid_predicted_wf)\n",
    "    print('session', i+1,'R2s:', R2s_wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_evaluate(num_units,frac_dropout,n_epochs):\n",
    "            num_units=int(num_units)\n",
    "            frac_dropout=float(frac_dropout)\n",
    "            n_epochs=int(n_epochs)\n",
    "            model_dnn=DenseNNDecoder(units=num_units,dropout=frac_dropout,num_epochs=n_epochs)\n",
    "            model_dnn.fit(X_train,y_train)\n",
    "            y_valid_predicted_dnn=model_dnn.predict(X_valid)\n",
    "            return np.mean(get_R2(y_valid,y_valid_predicted_dnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train_list)):   \n",
    "        print('session ',i+1)   \n",
    "\n",
    "        X_train = X_train_list[i].reshape(X_train_list[i].shape[0],(X_train_list[i].shape[1]*X_train_list[i].shape[2]))\n",
    "        X_valid = X_val_list[i].reshape(X_val_list[i].shape[0],(X_val_list[i].shape[1]*X_val_list[i].shape[2]))\n",
    "        y_train = y_train_list[i]\n",
    "        y_valid = y_val_list[i] \n",
    "        \n",
    "        #Do bayesian optimization\n",
    "        dnnBO = BayesianOptimization(dnn_evaluate, {'num_units': (50, 500), 'frac_dropout': (0,.5), 'n_epochs': (2,200)})\n",
    "        #lstmBO.maximize(init_points=20, n_iter=20, kappa=10)\n",
    "        utility = UtilityFunction(kind=\"ucb\", kappa=10, xi=0.0)\n",
    "        dnnBO.maximize(init_points=10,n_iter=5)\n",
    "\n",
    "        best_params=dnnBO.max['params']\n",
    "        frac_dropout=float(best_params['frac_dropout'])\n",
    "        n_epochs=int(best_params['n_epochs'])\n",
    "        num_units=int(best_params['num_units'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 #session\n",
    "\n",
    "X_flat_train=X_train_list[i].reshape(X_train_list[i].shape[0],(X_train_list[i].shape[1]*X_train_list[i].shape[2]))\n",
    "\n",
    "X_flat_test = X_test_list[i].reshape(X_test_list[i].shape[0],(X_test_list[i].shape[1]*X_test_list[i].shape[2]))\n",
    "\n",
    "model_dnn=DenseNNDecoder(units=200,dropout=0.25,num_epochs=5)\n",
    "\n",
    "#Fit model\n",
    "model_dnn.fit(X_flat_train,y_train_list[i])\n",
    "\n",
    "#Get predictions\n",
    "y_test_predicted_dnn=model_dnn.predict(X_flat_test)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_dnn=get_R2(y_test_list[i],y_test_predicted_dnn)\n",
    "print('session', i+1,'R2s:', R2s_dnn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_evaluate(num_units,frac_dropout,n_epochs):\n",
    "            num_units=int(num_units)\n",
    "            frac_dropout=float(frac_dropout)\n",
    "            n_epochs=int(n_epochs)\n",
    "            model_lstm=LSTMDecoder(units=num_units,dropout=frac_dropout,num_epochs=n_epochs)\n",
    "            model_lstm.fit(X_train,y_train)\n",
    "            y_valid_predicted_lstm=model_lstm.predict(X_valid)\n",
    "            return np.mean(get_R2(y_valid,y_valid_predicted_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train_list)):\n",
    "        print('session ',i+1)\n",
    "        X_train = X_train_list[i]\n",
    "        X_valid = X_val_list[i]\n",
    "        y_train = y_train_list[i]\n",
    "        y_valid = y_val_list[i]\n",
    "                \n",
    "        #Do bayesian optimization\n",
    "        lstmBO = BayesianOptimization(lstm_evaluate, {'num_units': (50, 500), 'frac_dropout': (0,.5), 'n_epochs': (2,25)})\n",
    "        #lstmBO.maximize(init_points=20, n_iter=20, kappa=10)\n",
    "        utility = UtilityFunction(kind=\"ucb\", kappa=10, xi=0.0)\n",
    "        lstmBO.maximize(init_points=10,n_iter=5)\n",
    "\n",
    "        best_params=lstmBO.max['params']\n",
    "        frac_dropout=float(best_params['frac_dropout'])\n",
    "        n_epochs=int(best_params['n_epochs'])\n",
    "        num_units=int(best_params['num_units'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the performance in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session id\n",
    "i = 0\n",
    "\n",
    "model_lstm=LSTMDecoder(units=210,dropout=0.37,num_epochs=23)\n",
    "\n",
    "#Fit model\n",
    "model_lstm.fit(X_train_list[i],y_train_list[i])\n",
    "\n",
    "#Get predictions\n",
    "y_test_predicted_lstm=model_lstm.predict(X_test_list[i])\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_lstm=get_R2(y_test_list[i],y_test_predicted_lstm)\n",
    "print('session', i+1,'R2s:', R2s_lstm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
